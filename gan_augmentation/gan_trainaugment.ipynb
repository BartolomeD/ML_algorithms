{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create binary classification data\n",
    "x_train, y_train = make_classification(n_samples=2000, n_features=20, n_informative=10)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train)\n",
    "\n",
    "# split data by class\n",
    "x_train_0, y_train_0 = x_train[y_train == 0], y_train[y_train == 0]\n",
    "x_train_1, y_train_1 = x_train[y_train == 1], y_train[y_train == 1]\n",
    "\n",
    "# model params\n",
    "g_input_size = 20\n",
    "g_hidden_size = 150\n",
    "g_output_size = 20\n",
    "d_input_size = 20\n",
    "d_hidden_size = 50\n",
    "d_output_size = 1\n",
    "\n",
    "# training params\n",
    "batch_size = 20\n",
    "d_learning_rate = 2e-4\n",
    "g_learning_rate = 1e-4\n",
    "optim_betas = (0.9, 0.999)\n",
    "num_epochs = 2000\n",
    "print_interval = 100\n",
    "g_steps = 2\n",
    "\n",
    "# batch generators\n",
    "def batch_gen(batch_size, x, y=None):\n",
    "    \n",
    "    # create list of batches\n",
    "    size = x.shape[0]\n",
    "    idx_array = np.arange(size)\n",
    "    n_batch = int(np.ceil(size / float(batch_size)))\n",
    "    batches = [(int(i * batch_size), int(min(size, (i + 1) * batch_size))) for i in range(n_batch)]\n",
    "    \n",
    "    # use list of batches to yield from input data\n",
    "    for batch_index, (start, end) in enumerate(batches):\n",
    "        batch_ids = idx_array[start:end]\n",
    "        if y is not None:\n",
    "            yield Variable(torch.from_numpy(x[batch_ids])), Variable(torch.from_numpy(y[batch_ids]))\n",
    "        else:\n",
    "            yield Variable(torch.from_numpy(x[batch_ids]))\n",
    "\n",
    "# generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)\n",
    "    \n",
    "# discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        return F.sigmoid(self.map3(x))\n",
    "\n",
    "# loss fn\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# models to predict class 0\n",
    "G_0 = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "D_0 = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "d_opt_0 = optim.Adam(D_0.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
    "g_opt_0 = optim.Adam(G_0.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
    "\n",
    "# models to predict class 1\n",
    "G_1 = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "D_1 = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "d_opt_1 = optim.Adam(D_1.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
    "g_opt_1 = optim.Adam(G_1.parameters(), lr=g_learning_rate, betas=optim_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \td_real_loss: 0.5579\td_fake_loss: 0.729\tg_loss: 0.6619\n",
      "Epoch 100 \td_real_loss: 0.3329\td_fake_loss: 0.1904\tg_loss: 2.5496\n",
      "Epoch 200 \td_real_loss: 0.8028\td_fake_loss: 0.4723\tg_loss: 1.1961\n",
      "Epoch 300 \td_real_loss: 0.6949\td_fake_loss: 0.5818\tg_loss: 0.6019\n",
      "Epoch 400 \td_real_loss: 0.6896\td_fake_loss: 0.7037\tg_loss: 0.8548\n",
      "Epoch 500 \td_real_loss: 0.9074\td_fake_loss: 0.4758\tg_loss: 0.968\n",
      "Epoch 600 \td_real_loss: 0.6603\td_fake_loss: 0.6009\tg_loss: 0.839\n",
      "Epoch 700 \td_real_loss: 0.8257\td_fake_loss: 0.7165\tg_loss: 0.6754\n",
      "Epoch 800 \td_real_loss: 0.7572\td_fake_loss: 0.6193\tg_loss: 0.8509\n",
      "Epoch 900 \td_real_loss: 0.7763\td_fake_loss: 0.8032\tg_loss: 0.7087\n",
      "Epoch 1000 \td_real_loss: 0.4685\td_fake_loss: 0.6639\tg_loss: 0.6641\n",
      "Epoch 1100 \td_real_loss: 1.0368\td_fake_loss: 0.7257\tg_loss: 0.668\n",
      "Epoch 1200 \td_real_loss: 0.4567\td_fake_loss: 0.654\tg_loss: 0.8452\n",
      "Epoch 1300 \td_real_loss: 0.4706\td_fake_loss: 0.6806\tg_loss: 0.6873\n",
      "Epoch 1400 \td_real_loss: 0.4858\td_fake_loss: 0.6761\tg_loss: 0.8807\n",
      "Epoch 1500 \td_real_loss: 0.5962\td_fake_loss: 0.8289\tg_loss: 0.6464\n",
      "Epoch 1600 \td_real_loss: 0.6885\td_fake_loss: 0.684\tg_loss: 0.7345\n",
      "Epoch 1700 \td_real_loss: 0.6031\td_fake_loss: 0.6038\tg_loss: 0.736\n",
      "Epoch 1800 \td_real_loss: 0.6386\td_fake_loss: 0.8271\tg_loss: 0.6012\n",
      "Epoch 1900 \td_real_loss: 0.6633\td_fake_loss: 0.6746\tg_loss: 0.7082\n"
     ]
    }
   ],
   "source": [
    "# train class 0 generator\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # instantiate batch loaders\n",
    "    batch_loader = batch_gen(batch_size, x_train_0, y_train_0)\n",
    "    for x, y in batch_loader:\n",
    "        D_0.zero_grad()\n",
    "\n",
    "        #  train D on real class 0 data\n",
    "        d_real_decision = D_0(x.float())\n",
    "        d_real_error = loss(d_real_decision, Variable(torch.ones((len(x), 1)))) # ones = true\n",
    "        d_real_error.backward() # compute and store gradients, don't change params\n",
    "\n",
    "        #  train D on fake class 0 data\n",
    "        d_gen_input = Variable(torch.randn(len(x), g_input_size))\n",
    "        d_fake_data = G_0(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        d_fake_decision = D_0(d_fake_data)\n",
    "        d_fake_error = loss(d_fake_decision, Variable(torch.zeros((len(x), 1)))) # zeros = fake\n",
    "        d_fake_error.backward()\n",
    "\n",
    "        # optimize D params, based on stored gradients from backward() with real + fake\n",
    "        d_opt_0.step()\n",
    "\n",
    "        for _ in range(g_steps):\n",
    "            G_0.zero_grad()\n",
    "\n",
    "            gen_input = Variable(torch.randn(len(x), g_input_size))\n",
    "            g_fake_data = G_0(gen_input)\n",
    "            dg_fake_decision = D_0(g_fake_data)\n",
    "            g_error = loss(dg_fake_decision, Variable(torch.ones((len(x), 1))))\n",
    "            g_error.backward()\n",
    "            g_opt_0.step()\n",
    "    \n",
    "    if epoch % print_interval == 0:\n",
    "        print('Epoch {} \\td_real_loss: {}\\td_fake_loss: {}\\tg_loss: {}'.format(\n",
    "            epoch, \n",
    "            round(d_real_error.data[0], 4),\n",
    "            round(d_fake_error.data[0], 4),\n",
    "            round(g_error.data[0], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \td_real_loss: 0.6441\td_fake_loss: 0.7062\tg_loss: 0.683\n",
      "Epoch 100 \td_real_loss: 1.0428\td_fake_loss: 0.4461\tg_loss: 1.5447\n",
      "Epoch 200 \td_real_loss: 0.4023\td_fake_loss: 0.4523\tg_loss: 0.9873\n",
      "Epoch 300 \td_real_loss: 0.9556\td_fake_loss: 0.6086\tg_loss: 0.819\n",
      "Epoch 400 \td_real_loss: 0.5581\td_fake_loss: 0.6403\tg_loss: 0.7663\n",
      "Epoch 500 \td_real_loss: 0.5373\td_fake_loss: 0.8199\tg_loss: 0.6224\n",
      "Epoch 600 \td_real_loss: 0.7304\td_fake_loss: 0.4689\tg_loss: 0.9775\n",
      "Epoch 700 \td_real_loss: 0.9036\td_fake_loss: 0.7135\tg_loss: 0.7506\n",
      "Epoch 800 \td_real_loss: 0.6516\td_fake_loss: 0.5061\tg_loss: 0.9862\n",
      "Epoch 900 \td_real_loss: 0.8509\td_fake_loss: 0.4352\tg_loss: 1.0231\n",
      "Epoch 1000 \td_real_loss: 0.7484\td_fake_loss: 0.6967\tg_loss: 0.6285\n",
      "Epoch 1100 \td_real_loss: 0.6737\td_fake_loss: 0.8008\tg_loss: 0.623\n",
      "Epoch 1200 \td_real_loss: 0.5728\td_fake_loss: 0.5715\tg_loss: 0.8846\n",
      "Epoch 1300 \td_real_loss: 0.6785\td_fake_loss: 0.7411\tg_loss: 0.7032\n",
      "Epoch 1400 \td_real_loss: 0.5944\td_fake_loss: 0.8443\tg_loss: 0.575\n",
      "Epoch 1500 \td_real_loss: 0.5986\td_fake_loss: 0.6502\tg_loss: 0.7368\n",
      "Epoch 1600 \td_real_loss: 0.6833\td_fake_loss: 0.7646\tg_loss: 0.6582\n",
      "Epoch 1700 \td_real_loss: 0.6353\td_fake_loss: 0.5277\tg_loss: 0.8989\n",
      "Epoch 1800 \td_real_loss: 0.7176\td_fake_loss: 0.7397\tg_loss: 0.68\n",
      "Epoch 1900 \td_real_loss: 0.6929\td_fake_loss: 0.6651\tg_loss: 0.7854\n"
     ]
    }
   ],
   "source": [
    "# train class 1 generator\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # instantiate batch loaders\n",
    "    batch_loader = batch_gen(batch_size, x_train_1, y_train_1)\n",
    "    for x, y in batch_loader:\n",
    "        \n",
    "        D_1.zero_grad()\n",
    "\n",
    "        #  train D on real class 0 data\n",
    "        d_real_decision = D_1(x.float())\n",
    "        d_real_error = loss(d_real_decision, Variable(torch.ones((len(x), 1)))) # ones = true\n",
    "        d_real_error.backward() # compute and store gradients, don't change params\n",
    "\n",
    "        #  train D on fake class 0 data\n",
    "        d_gen_input = Variable(torch.randn(len(x), g_input_size))\n",
    "        d_fake_data = G_1(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        d_fake_decision = D_1(d_fake_data)\n",
    "        d_fake_error = loss(d_fake_decision, Variable(torch.zeros((len(x), 1)))) # zeros = fake\n",
    "        d_fake_error.backward()\n",
    "\n",
    "        # optimize D params, based on stored gradients from backward() with real + fake\n",
    "        d_opt_1.step()\n",
    "\n",
    "        for _ in range(g_steps):\n",
    "            G_1.zero_grad()\n",
    "\n",
    "            gen_input = Variable(torch.randn(len(x), g_input_size))\n",
    "            g_fake_data = G_1(gen_input)\n",
    "            dg_fake_decision = D_1(g_fake_data)\n",
    "            g_error = loss(dg_fake_decision, Variable(torch.ones((len(x), 1))))\n",
    "            g_error.backward()\n",
    "            g_opt_1.step()\n",
    "    \n",
    "    if epoch % print_interval == 0:\n",
    "        print('Epoch {} \\td_real_loss: {}\\td_fake_loss: {}\\tg_loss: {}'.format(\n",
    "            epoch, \n",
    "            round(d_real_error.data[0], 4),\n",
    "            round(d_fake_error.data[0], 4),\n",
    "            round(g_error.data[0], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.89      0.89       260\n",
      "          1       0.88      0.89      0.88       240\n",
      "\n",
      "avg / total       0.89      0.89      0.89       500\n",
      "\n",
      "0.888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# use logistic regression on original data\n",
    "model = LogisticRegression()\n",
    "preds = model.fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "# print results\n",
    "print(classification_report(preds, y_test))\n",
    "print(accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.88      0.87       252\n",
      "        1.0       0.88      0.85      0.87       248\n",
      "\n",
      "avg / total       0.87      0.87      0.87       500\n",
      "\n",
      "0.868\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n):\n",
    "    # generate synthetic data\n",
    "    xf_train_0 = G_0(Variable(torch.randn(n, 20))).data.numpy()\n",
    "    xf_train_1 = G_1(Variable(torch.randn(n, 20))).data.numpy()\n",
    "\n",
    "    # create beefed up trainset\n",
    "    x_train_syn = np.vstack([x_train_0, xf_train_0, x_train_1, xf_train_1])\n",
    "    y_train_syn = np.hstack([np.zeros((len(x_train_0) + n)), np.ones((len(x_train_1) + n))]).reshape(-1, 1)\n",
    "    \n",
    "    # shuffle data\n",
    "    data = np.hstack([x_train_syn, y_train_syn])\n",
    "    data = pd.DataFrame(data).iloc[np.random.permutation(len(data))]\n",
    "    return data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
    "\n",
    "# generate synthetic data\n",
    "x_train_syn, y_train_syn = generate_data(500)\n",
    "\n",
    "# use logistic regression on augmented data\n",
    "preds = model.fit(x_train_syn, y_train_syn).predict(x_test)\n",
    "\n",
    "# print results\n",
    "print(classification_report(preds, y_test))\n",
    "print(accuracy_score(preds, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
